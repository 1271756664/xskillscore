{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "See the API for more detailed information, examples, formulas, and references for each function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xskillscore as xs\n",
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate some sample gridded data. Our data has three time steps, and a 4x5 latitude/longitude grid. `obs` replicates some verification data and `fct` some forecast (e.g. from a statistical or dynamical model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = xr.DataArray(\n",
    "       np.random.rand(3, 4, 5),\n",
    "       coords=[\n",
    "           xr.cftime_range(\"2000-01-01\", \"2000-01-03\", freq=\"D\"),\n",
    "           np.arange(4),\n",
    "           np.arange(5),\n",
    "       ],\n",
    "       dims=[\"time\", \"lat\", \"lon\"],\n",
    "       name='var'\n",
    "   )\n",
    "fct = obs.copy()\n",
    "fct.values = np.random.rand(3, 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Metrics\n",
    "\n",
    "`xskillscore` offers a suite of correlation-based and distance-based deterministic metrics.\n",
    "\n",
    "### Correlation-Based \n",
    "\n",
    "* Pearson Correlation (`pearson_r`)\n",
    "* Pearson Correlation p value (`pearson_r_p_value`)\n",
    "* Pearson Correlation effective p value (`pearson_r_eff_p_value`)\n",
    "* Spearman Correlation (`spearman_r`)\n",
    "* Spearman Correlation p value (`spearman_r_p_value`)\n",
    "* Spearman Correlation effective p value (`spearman_r_eff_p_value`)\n",
    "* Effective Sample Size (`effective_sample_size`)\n",
    "* Coefficient of Determination (`r2`)\n",
    "\n",
    "### Distance-Based\n",
    "\n",
    "* Mean Error (`me`)\n",
    "* Root Mean Squared Error (`rmse`)\n",
    "* Mean Squared Error (`mse`)\n",
    "* Mean Absolute Error (`mae`)\n",
    "* Median Absolute Error (`median_absolute_error`)\n",
    "* Symmetric Mean Absolute Percentage Error (`smape`)\n",
    "* Mean Absolute Percentage Error (`mape`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the functions is very straight-forward. All deterministic functions take the form `func(a, b, dim=None, **kwargs)`. **Notice that the original dataset is reduced by the dimension passed.** I.e., since we passed `time` as the dimension here, we are returned an object with dimensions `(lat, lon)`. For correlation metrics `dim` cannot be `[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = xs.pearson_r(obs, fct, dim='time')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = xs.pearson_r_p_value(obs, fct, dim=\"time\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also specify multiple axes for deterministic metrics. Here, we apply it over the latitude and longitude dimension (a pattern correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = xs.pearson_r(obs, fct, dim=[\"lat\", \"lon\"])\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All deterministic metrics except for `pearson_r_eff_p_value`, `spearman_r_eff_p_value`, and `effective_sample_size` can take the kwarg `weights=...`. `weights` should be a DataArray of the size of the reduced dimension (e.g., if time is being reduced it should be of length 3 in our example).\n",
    "\n",
    "Weighting is a common practice when working with observations and model simulations of the Earth system. When working with rectilinear grids, one can weight the data by the cosine of the latitude, which is maximum at the equator and minimum at the poles (as in the below example). More complicated model grids tend to be accompanied by a cell area varaible, which could also be passed into this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs2 = xr.DataArray(\n",
    "       np.random.rand(3, 180, 360),\n",
    "       coords=[\n",
    "           xr.cftime_range(\"2000-01-01\", \"2000-01-03\", freq=\"D\"),\n",
    "           np.linspace(-89.5, 89.5, 180),\n",
    "           np.linspace(-179.5, 179.5, 360),\n",
    "       ],\n",
    "       dims=[\"time\", \"lat\", \"lon\"],\n",
    "    )\n",
    "fct2 = obs2.copy()\n",
    "fct2.values = np.random.rand(3, 180, 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make weights as cosine of the latitude and broadcast\n",
    "weights = np.cos(np.deg2rad(obs2.lat))\n",
    "_, weights = xr.broadcast(obs2, weights)\n",
    "\n",
    "# Remove the time dimension from weights\n",
    "weights = weights.isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_weighted = xs.pearson_r(obs2, fct2, dim=[\"lat\", \"lon\"], weights=weights)\n",
    "print(r_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_unweighted = xs.pearson_r(obs2, fct2, dim=[\"lat\", \"lon\"], weights=None)\n",
    "print(r_unweighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass the optional boolean kwarg `skipna=...`. If `True`, ignore any NaNs (pairwise) in `a` and `b` when computing the result. If `False`, return NaNs anywhere there are pairwise NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_with_nans = obs.where(obs.lat > 1)\n",
    "fct_with_nans = fct.where(fct.lat > 1)\n",
    "print(obs_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_with_skipna = xs.mae(obs_with_nans, fct_with_nans, dim=['lat', 'lon'], skipna=True)\n",
    "print(mae_with_skipna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_without_skipna = xs.mae(obs_with_nans, fct_with_nans, dim=['lat', 'lon'], skipna=False)\n",
    "print(mae_without_skipna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Metrics\n",
    "\n",
    "`xskillscore` offers a suite of probabilistic metrics, mostly ported from `properscoring`.\n",
    "\n",
    "* Continuous Ranked Probability Score with the ensemble distribution (`crps_ensemble`)\n",
    "* Continuous Ranked Probability Score with a Gaussian distribution (`crps_gaussian`)\n",
    "* Continuous Ranked Probability Score with numerical integration of the normal distribution (`crps_quadrature`)\n",
    "* Brier scores of an ensemble for exceeding given thresholds (`threshold_brier_score`)\n",
    "* Brier Score (`brier_score`)\n",
    "* Ranked Probability Score (`rps`)\n",
    "* Discrimination (`discrimination`)\n",
    "* Rank Histogram (`rank_histogram`)\n",
    "* Reliability (`reliability`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create some data with an ensemble member dimension. In this case, we envision an ensemble forecast with multiple members to validate against our theoretical observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs3 = xr.DataArray(\n",
    "       np.random.rand(4, 5),\n",
    "       coords=[np.arange(4), np.arange(5)],\n",
    "       dims=[\"lat\", \"lon\"],\n",
    "       name='var'\n",
    "   )\n",
    "fct3 = xr.DataArray(\n",
    "       np.random.rand(3, 4, 5),\n",
    "       coords=[np.arange(3), np.arange(4), np.arange(5)],\n",
    "       dims=[\"member\", \"lat\", \"lon\"],\n",
    "       name='var'\n",
    "   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous Ranked Probability Score with the ensemble distribution. Pass `dim=[]` to get the same behaviour as `properscoring.crps_ensemble` without any averaging over `dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_ensemble = xs.crps_ensemble(obs3, fct3, dim=[])\n",
    "print(crps_ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRPS with a Gaussian distribution requires two parameters: $\\mu$ and $\\sigma$ from the forecast distribution. Here, we just use the ensemble mean and ensemble spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_gaussian = xs.crps_gaussian(obs3, fct3.mean(\"member\"), fct3.std(\"member\"), dim=[])\n",
    "print(crps_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRPS quadrature metric requires a callable distribution function. Here we use `norm` from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "crps_quadrature = xs.crps_quadrature(obs3, norm, dim=[])\n",
    "print(crps_quadrature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a threshold Brier Score, to score hits over a certain threshold. Ranked Probability Score for two categories yields the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_brier_score = xs.threshold_brier_score(obs3, fct3, 0.5, dim=None)\n",
    "print(threshold_brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score = xs.brier_score(obs3>.5, (fct3>.5).mean('member'))\n",
    "print(brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rps = xs.rps(obs3, fct3, category_edges=np.array([0.0, 0.5, 1.0]))\n",
    "print(rps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_histogram = xs.rank_histogram(obs3, fct3)\n",
    "print(rank_histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = xs.discrimination(obs3 > 0.5, (fct3 > 0.5).mean(\"member\"))\n",
    "print(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = xs.reliability(obs3 > 0.5, (fct3 > 0.5).mean(\"member\"))\n",
    "print(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contingency-Based\n",
    "\n",
    "To work with contingency-based scoring, first instantiate a `Contingency` object by passing in your observations, forecast, and observation/forecast bin edges. See https://www.cawcr.gov.au/projects/verification/#Contingency_table for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dichotomous_category_edges = np.array([0, 0.5, 1]) # \"dichotomous\" mean two-category\n",
    "dichotomous_contingency = xs.Contingency(obs, fct,\n",
    "                                        dichotomous_category_edges,\n",
    "                                        dichotomous_category_edges,\n",
    "                                        dim=['lat','lon'])\n",
    "dichotomous_contingency_table = dichotomous_contingency.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dichotomous_contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dichotomous_contingency_table.to_dataframe().pivot_table(index=['forecasts_category','forecasts_category_bounds'],\n",
    "                                                         columns=['observations_category','observations_category_bounds']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores based on the constructed contingency table can be called via class methods. The available methods are:\n",
    "\n",
    "* `bias_score`\n",
    "* `hit_rate`\n",
    "* `false_alarm_ratio`\n",
    "* `false_alarm_rate`\n",
    "* `success_ratio`\n",
    "* `threat_score`\n",
    "* `equit_threat_score`\n",
    "* `odds_ratio`\n",
    "* `odds_ratio_skill_score`\n",
    "* `accuracy`\n",
    "* `heidke_score`\n",
    "* `peirce_score`\n",
    "* `gerrity_score`\n",
    "\n",
    "Below, we share a few examples of these in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dichotomous_contingency.bias_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dichotomous_contingency.hit_rate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dichotomous_contingency.false_alarm_rate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dichotomous_contingency.odds_ratio_skill_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage multi-category edges to make use of some scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_category_edges = np.array([0, 0.25, 0.75, 1])\n",
    "multicategory_contingency = xs.Contingency(obs, fct,\n",
    "                                              multi_category_edges,\n",
    "                                              multi_category_edges,\n",
    "                                              dim=['lat','lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multicategory_contingency.accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multicategory_contingency.heidke_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multicategory_contingency.peirce_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multicategory_contingency.gerrity_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative\n",
    "\n",
    "Test to compare whether one forecast is significantly better than another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=100\n",
    "obs_1d = xr.DataArray(\n",
    "       np.random.rand(length),\n",
    "       coords=[\n",
    "           np.arange(length),\n",
    "       ],\n",
    "       dims=[\"time\"],\n",
    "       name='var'\n",
    "   )\n",
    "fct_1d = obs_1d.copy()\n",
    "fct_1d.values = np.random.rand(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given you want to test whether one forecast is better than another forecast\n",
    "sign_test = xs.sign_test(fct_1d, fct_1d+.2, obs_1d, time_dim='time', metric='mae', orientation='negative')\n",
    "print(sign_test.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_test.plot()\n",
    "sign_test.confidence.plot(c='gray')\n",
    "(-1*sign_test.confidence).plot(c='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessors\n",
    "\n",
    "You can also use `xskillscore` as a method of your `xarray` Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset()\n",
    "ds[\"obs_var\"] = obs\n",
    "ds[\"fct_var\"] = fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case that your Dataset contains both your observation and forecast variable, just pass them as strings into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.xs.pearson_r(\"obs_var\", \"fct_var\", dim=\"time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in a separate Dataset that contains your observations or forecast variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.drop_vars(\"fct_var\")\n",
    "print(ds.xs.pearson_r(\"obs_var\", fct, dim=\"time\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
